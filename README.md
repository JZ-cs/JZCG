# A few words
&emsp;&emsp;依稀记得在当时写个全连接的反向传播应该算是深度学习的"Hello World"吧。 

&emsp;&emsp;这是一个很简单的框架，很多功能都待完善和添加。在有像PyTorch, TensorFlow等这么成熟和完善的框架以及各大公司也有各自自研框架的现在，重复造这个轮子有什么意义呢？从有这个打算，到目前自己花了不少时间在这上面，偶尔有空就会写一写，而时间总是有限的，现在正是校招的时候，时间尤其宝贵，想到这里我觉得自己还是不太明智的，纯兴趣的东西应当在自己空闲时折腾，所以这对我而言既是收获，也是教训。收获是一路写下来学到了很多，教训嘛，就是什么时候该干什么自己应该好好搞清楚。当然，现在是人工智能的时代，我觉得每个人都可以借助实现这样一个玩具，加深下深度学习的基础。因为写到现在，我越发觉得基本上只要想写，肯投入时间进去都能做到，不管有没有基础。我想对于刚刚打算进入深度学习的人而言，这块应该是跑不了的，就算不是自己去实现个简单的，也要通过其他方式。很多时候有些问题只有自己去设计实现才会考虑到，比如计算图该组织成什么形式？多维张量怎么实现，维度不确定的情况下不能简单地定义2维3维数组来存数据，如果模仿内存采用1维连续数组，怎么对外展现出它是个有自己shape的张量？这就使得原本十分简单的+-*/操作变得很复杂，同时还得考虑满足一定条件的情况下，张量之间是可以广播的，这时该怎么操作，存在广播的情况下，反向求导时又该怎么计算？怎么定义Module模块？怎么界定哪些部分是Module内部的，哪些是外部的，反向传播时该怎么做？因为一个CG中很可能是Module套Module（也就是套娃），因为不这样怎么支持卷积层，全连接层乃至现有模型等自定义模块？自己很多时候并不在乎对错和效率，只是觉得自己慢慢去实现一种思路的过程很有意思，也正因为如此，如果各位在代码里看到了很sb的操作，不要觉得奇怪，因为我只是偶尔参考现有框架的设计思路，但没有参考阅读源码，所以很多地方难免会写得不怎么优雅甚至可能是个BUG。

&emsp;&emsp;代码不多，几千行，使用Java编写(当时主要是想熟悉下Java)，早期是VSCode搭的环境，后期转到IDEA了。

### 已实现的如下：
1. 基本操作：加减乘除和矩阵乘法，已经放弃的dot操作;
2. reduce操作：sum, max(min还没写^_^);
3. 功能性操作：transpose, squeeze, unsqueeze;
4. 函数操作实现比较简单，因此只实现了exp, log, sigmoid, 后续可以轻易添加其他函数实现；
5. 损失函数目前只有MSE，后续有空会添加其他的。
6. 一个简单的Module模块，目的在于后续复杂的实现比如全连接层，卷积层以及自定义的模型都应该在继承它的基础上实现，这样只需要实现模型的搭建，完成前向传播的逻辑，类似PyTorch, 不需要考虑反向传播的问题。

&emsp;&emsp;所有已经实现的操作都支持多维张量，也都支持广播操作。

### 目前已实现但还未完善：
1.  单核的训练方式，多线程模型一个简单的思路是构建一个DAG的degree table, 叶子结点的degree均为0，根据拓扑排序往上递增，这时呈现一个金子塔的层级结构，同一层的均可以同时计算，下一层的可以不断去遍历，在当前层的部分完成后也可以启动计算。
1.  数据分布式训练, 整个模型可以容纳在一台机器内，只做数据上的并行训练，训练过程中梯度交换策略为Ring Allreduce，只在3台linux云服务器或者轻量应用服务器上测试过，其中网络传输梯度完全可以继续优化。
2.  模型分布式训练，分散整个模型在不同机器训练，只实现了部分，主要瓶颈和未实现的部分在于: 怎么更好地分割整个模型，尽可能满足负载均衡，提升并行度和减少不必要的网络传输， 以及后续的高可用。

### 大佬轻喷，欢迎提出意见，不论是关于Java的，还是实现上的，直接拉分支当然更好。

![示例](https://github.com/JZ-cs/JZCG/images/CG example.pdf)




